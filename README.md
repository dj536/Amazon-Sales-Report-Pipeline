# ğŸ“Š Amazon Sales Report - End-to-End DLT Pipeline

Ce projet implÃ©mente un pipeline pipeline Lakehouse entiÃ¨rement automatisÃ© permettant dâ€™ingÃ©rer, transformer, historiser et analyser des donnÃ©es de ventes Amazon, en appliquant les principes Bronze â†’ Silver â†’ Gold, lâ€™orchestration moderne avec LakeFlow Declarative Pipelines, et la modÃ©lisation analytique Data Warehouse (dimensions + fact table).


## ğŸš€ Architecture du Projet

Le pipeline repose sur l'architecture Medallion utilisant Delta Live Tables (DLT) :

Couche Bronze : Ingestion brute via cloudFiles depuis un Volume Unity Catalog; utilisation de l'Auto Loader de Databricks. Contrairement Ã  une ingestion classique, Auto Loader dÃ©tecte automatiquement les nouveaux fichiers CSV arrivant dans le Volume Unity Catalog et ne traite que les nouveautÃ©s (incrÃ©mental). On dÃ©finit Ã©galement les rÃ¨gles (rule1, rule2...) qui marquent les lignes suspectes

Couche Silver : Nettoyage, enrichissement temporel et agrÃ©gation, elle contient des donnÃ©es propres, filtrÃ©es , filtrÃ©es, transformÃ©es, aggrÃ©gÃ©es et prÃªtes pour le DWH.

Couche Gold : ModÃ©lisation en SchÃ©ma en Ã‰toile avec gestion de l'historique (SCD Type 2) et ClÃ©s de Substitution, la couche finale transforme les flux de donnÃ©es en un SchÃ©ma en Ã‰toile (Star Schema), pour l'aide Ã  la dÃ©cision.

## ğŸ› ï¸ Stack Technique

Plateforme : Databricks (Azure/AWS) 

Moteur de DonnÃ©es : Delta Live Tables (DLT)

Langages : PySpark (ETL) & SQL (Reporting)

Gouvernance : Unity Catalog


## ğŸ“ Structure des DonnÃ©es (Star Schema)

Le modÃ¨le dimensionnel final permet des analyses croisÃ©es sur 5 perspectives clÃ©s:

Fact_Sales : SalesKey, ProductKey, TimeKey, LocationKey, SalesChannelKey, OrderStatusKey, Qty

Dim_Product : SKU, Style, Category, Size, ProductCode, Line 

Dim_Time : Date, Day, Month, Quarter, Year, Week, TimeKey 

Dim_Location : ShipState, ShipPostalCode, ShipCountry, location_key 

Dim_Sales_Channel : fulfilmentType, servicelevel, channelKey 

Dim_Order_Status : orderStatus, StatusCategory, status_key


## ğŸ“‚ Project Structure
```
â”œâ”€â”€ explorations/
â”‚   â””â”€â”€ sample_explorations.py       
â”œâ”€â”€ transformations/
â”‚   â”œâ”€â”€ Bronze/
â”‚   â”‚   â””â”€â”€ data_ingestion.py        
â”‚   â”œâ”€â”€ Silver/
â”‚   â”‚   â”œâ”€â”€ amazon_strmg.py        
â”‚   â”‚   â””â”€â”€ aggregation.py     
â”‚   â””â”€â”€ Gold/
â”‚       â”œâ”€â”€ Fact_layer.py       
â”‚       â””â”€â”€dim Order_status.py
â”‚       â””â”€â”€ dim location.py
â”‚       â””â”€â”€ dim product.py
â”‚       â””â”€â”€ dim time.py
â”‚       â””â”€â”€ dim_salesChannel.py        
â”œ  
â”œâ”€â”€ volumes/
â”‚   â””â”€â”€ data/                   
â””â”€â”€ README.md                   
```


## ğŸ› ï¸ Getting Started

Databricks Workspace: Un compte actif avec un cluster fonctionnel:
```
https://www.databricks.com/fr/learn/free-edition
```
