# ğŸ“Š Amazon Sales Report - End-to-End DLT Pipeline

Ce projet implÃ©mente un pipeline pipeline Lakehouse entiÃ¨rement automatisÃ© permettant dâ€™ingÃ©rer, transformer, historiser et analyser des donnÃ©es de ventes Amazon, en appliquant les principes Bronze â†’ Silver â†’ Gold, lâ€™orchestration moderne avec LakeFlow Declarative Pipelines, et la modÃ©lisation analytique Data Warehouse (dimensions + fact table).


## ğŸš€ Architecture du Projet

Le pipeline repose sur l'architecture Medallion utilisant Delta Live Tables (DLT) :
Couche Bronze : Ingestion brute via cloudFiles depuis un Volume Unity Catalog. 
Couche Silver : Nettoyage, enrichissement temporel et agrÃ©gation. 
Couche Gold : ModÃ©lisation en SchÃ©ma en Ã‰toile avec gestion de l'historique (SCD Type 2) et ClÃ©s de Substitution.

## ğŸ› ï¸ Stack Technique

Plateforme : Databricks (Azure/AWS) 

Moteur de DonnÃ©es : Delta Live Tables (DLT)

Langages : PySpark (ETL) & SQL (Reporting)

Gouvernance : Unity Catalog


ğŸ“ Structure des DonnÃ©es (Star Schema)

Le modÃ¨le dimensionnel final permet des analyses croisÃ©es sur 5 perspectives clÃ©s:

Fact_Sales : SalesKey, ProductKey, TimeKey, LocationKey, SalesChannelKey, OrderStatusKey, Qty

Dim_Product : SKU, Style, Category, Size, ProductCode, Line 

Dim_Time : Date, Day, Month, Quarter, Year, Week, TimeKey 

Dim_Location : ShipState, ShipPostalCode, ShipCountry, location_key 

Dim_Sales_Channel : fulfilmentType, servicelevel, channelKey 

Dim_Order_Status : orderStatus, StatusCategory, status_key
